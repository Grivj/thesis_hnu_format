\begin{abstract}
	\large
	% 与其他机器学习算法一样，神经网络很容易受到对抗性示例的影响，即包含特制扰
	% 动的输入，其唯一目的是欺骗网络进行错误分类。另一方面，由于对神经网络架构
	% 的改进，众所周知，它们对输入中的随机扰动更加稳健。这种对随机扰动的鲁棒性
	% 促使我提出了一种易于部署的对抗样本检测方法，该方法可以测量将随机噪声应用
	% 于输入图像之前和之后的预测不一致性。在三个流行基准 (Dogs vs.
	% Cats、CIFAR-10 和 ImageNet) 的子集上评估该方法表明，它在针对更高分辨率图
	% 像的各种攻击中实现了高对抗样本检测性能。我的方法的主要优点是简单，计算成
	% 本低，并且不需要任何关于所使用攻击的先验知识，这使得我的方法很容易集成到
	% 其他防御框架中.

	% 我提出了一种检测对抗样本的新方法，该方法基于在输入上故意引入不同强度的高斯噪
	% 声。然后，计算两个分数，以评估模型在应用噪声之前和之后在不同强度下的预测差
	% 异。这种方法的优点是检测效率不依赖于关于所使用攻击的先验知识，因此可以应用于
	% 广泛的攻击和不同的对抗性扰动预算。此外，与最先进的防御或检测方法相反，我的方
	% 法对计算的要求很低，因为不需要对模型参数进行训练或优化。最后，此方法可以与其
	% 他检测方法结合使用，以提高整体应用程序的性能。我提出的方法是在研究了将加性高
	% 斯噪声应用于正常图像和对抗样本的影响并观察到据我所知并且在这项工作时尚未在之
	% 前的工作中讨论过的差异之后出现的。

	分类器和其他模型现在在许多领域都实现了显着的准确性，因此被部署在现实生活场景
	中，例如自动驾驶汽车、欺诈检测或情绪分析。这项研究的重点是图像分类，这是深度
	学习最近使用的亮点之一。该领域中最先进的模型现在可以在具有挑战性的任务上 (例
	如大规模 Imagenet 数据集上的分类) 实现与人类相等级别的准确性。

	不幸的是，与其他机器学习算法一样，神经网络已被证明容易受到对抗样本的影响，即
	包含特制扰动的输入，其唯一目的是欺骗网络进行错误分类。此外，这些精心设计的扰
	动可能非常小，以至于人类观察者无法察觉，这表明尽管这些最近的模型在分类任务中
	可以与人类一样准确，但它们并不可靠。

	另一方面，由于对神经网络架构的不断研究和改进，最近的模型对输入中的随机扰动
	(例如噪声、模糊或压缩) 更加稳健。这种对随机扰动的鲁棒性促使研究像素空间扰动
	对典型图像 (即良性) 和具有不同噪声和对抗性扰动强度的对抗样本的影响。直觉是，
	对抗性扰动可能不像自然图像那样抗劣化，因为它们可能不具有相同的属性或内在含
	义。

	据本人所知，本文在这项研究中分享的实验和结果并没有在以前的文献中展示过，并且
	证实了前面提到的直觉。从实验中获得的结果使本文提出了一种检测这些对抗样本的新
	方法。该方法的本质是故意在输入图像上引入不同强度的高斯噪声，以计算两个指标，
	从而评估添加扰动前后的预测的差异。本文证明，对抗样本的差异通常比使用三个流行
	基准 (Dogs vs. Cats、CIFAR-10 和 ImageNet) 的标准图像要高得多。

	本文的方法的主要优点是其简单性和低计算成本，使其易于部署和与其他防御框架结
	合。最重要的是，无论对攻击的任何先验知识或其对抗性预算如何，该方法都显示出对
	高分辨率图像的各种攻击的高检测性能。
	\keywords{深度学习；计算机视觉；对抗性例子}
\end{abstract}
\afterpage{\blankpage}

\begin{enabstract}
	\large
	Like other machine learning algorithms, neural networks have been vulnerable
	to adversarial examples, i.e., inputs containing specifically crafted
	perturbations whose only objective is to fool a network into
	misclassification. On the other hand, thanks to the continuous research and
	improvements made to neural network architectures, recent models are much
	more robust to random perturbations in the input. This robustness to random
	perturbations motivated the study of the effect of pixel-space perturbations
	on typical images (i.e., benign) and adversarial examples with varying
	intensities of noise and adversarial perturbation.

	These experiments, followed by the results described in this research, led
	me to propose a novel method to detect adversarial examples. The essence of
	the method is to deliberately introduce Gaussian noise of varying strengths
	on the input images to compute two metrics that evaluate the differences in
	predictions ante and post the added perturbations. Differences that I prove
	to be typically much higher with adversarial examples than with standard
	images using three popular benchmarks (Dogs vs. Cats, CIFAR-10, and
	ImageNet).

	The main advantages of my approach are its simplicity and low computational
	cost, making it easy to deploy and combine with other defense frameworks.
	Most importantly, the method shows high detection performance on various
	attacks on higher-resolution images regardless of any prior knowledge of the
	attack or its adversarial budget.
	\enkeywords{Deep Learning; Computer Vision; Adversarial Examples}
\end{enabstract}
\afterpage{\blankpage}
