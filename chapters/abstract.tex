\begin{abstract}

	分类器和其他模型现在在许多领域中都实现了高度的准确性，因此广泛的应用在现实生
	活场景中，例如自动驾驶汽车、欺诈检测或情绪分析。这项研究的重点是图像分类，这
	是深度学习最近使用的亮点之一。该领域中最先进的模型现在可以在具有挑战性的任务
	上 (例如大规模 Imagenet 数据集上的分类) 实现与人类相等级别的准确性。

	不幸的是，与其他机器学习算法一样，神经网络已被证明容易受到对抗样本的影响，即
	包含特制扰动的输入，其唯一目的是欺骗网络进行错误分类。此外，这些精心设计的扰
	动可能非常小，以至于人类观察者无法察觉，这表明尽管这些最近的模型在分类任务中
	可以与人类一样准确，但它们并不可靠。

	另一方面，由于对神经网络架构的不断研究和改进，最近的模型对输入中的随机扰动
	(例如噪声、模糊或压缩) 更加稳健。这种对随机扰动的鲁棒性促使研究像素空间扰动
	对典型图像 (即良性) 和具有不同噪声和对抗性扰动强度的对抗样本的影响。直觉是，
	对抗性扰动可能不像自然图像那样抗劣化，因为它们可能不具有相同的属性或内在含
	义。

	据本人所知，本文在这项研究中分享的实验和结果并没有在以前的文献中展示过，并且
	证实了前面提到的直觉。从实验中获得的结果使本文提出了一种检测这些对抗样本的新
	方法。该方法的本质是故意在输入图像上引入不同强度的高斯噪声，以计算两个指标，
	从而评估添加扰动前后的预测的差异。本文证明，对抗样本的差异通常比使用三个流行
	基准 (Dogs vs. Cats、CIFAR-10 和 ImageNet) 的标准图像要高得多。

	本文的方法的主要优点是其简单性和低计算成本，使其易于部署和与其他防御框架结
	合。最重要的是，无论对攻击的任何先验知识或其对抗性预算如何，该方法都显示出对
	高分辨率图像的各种攻击的高检测性能。

	图像分类和其他模型在许多领域的应用已经取得了显著的进展，而深度学习技术在这一
	领域的发展则为人工智能的广泛应用提供了强大的支持。尽管如此，现有的深度学习方
	法在处理对抗样本方面仍存在一定的局限性，对此，本文旨在研究对抗样本的特点以及
	如何在实际应用中提高模型的鲁棒性。

	在过去的几年里，许多研究人员在图像分类任务上取得了令人瞩目的成果。这些成功的
	关键在于深度神经网络的发展，尤其是卷积神经网络 (CNN)。然而，尽管神经网络在图
	像分类任务中的表现逐渐逼近人类水平，但对抗性攻击仍然是一个悬而未决的问题。这
	些但对抗性攻击仍然是一个悬而未决的问题。这些攻击采用微小的、人眼难以察觉的扰
	动，试图迷惑神经网络进行错误分类。针对这一问题，本研究力图在提高模型鲁棒性的
	同时，提出一种有效检测对抗性样本的方法。

	为了理解对抗样本的特点及其对分类器性能的影响，本文首先对图像分类任务的基本概
	念和现有方法进行了概述。接着，我们探讨了不同类型的扰动，包括随机扰动和对抗性
	扰动，以及它们在分类任务中的应用。此外，我们还分析了现有文献中的对抗性攻击方
	法，如 FGSM、PGD 和 CW 等，以便更好地了解对抗样本的生成过程。

	为了实证研究对抗样本的特性，本文设计了一系列实验，旨在评估不同类型扰动下模型
	的性能变化。我们使用了 Dogs vs. Cats、CIFAR-10 和 ImageNet 这三个流行的基准
	数据集进行实验。实验结果表明，对抗性扰动确实会显著降低模型的分类性能，而随机
	扰动对模型的影响相对较小。

	基于实验结果，我们提出了一种新的对抗样本检测方法，其核心思想是在输入图像上有
	意地引入不同强度的高斯噪声，以计算扰动前后预测的差异。我们发现，对抗样本的差
	异通常远高于标准图像。通过使用这种方法，我们可以在保持较低计算成本的同时，有
	效地检测出对抗样本。

	在评估了所提方法的有效性后，我们还讨论了如何将其应用于实际场景，包括自动驾驶
	汽车、欺诈检测和情绪分析等领域。我们认为，本文提出的方法可以作为一种补充手段

	本文提出的方法可以作为一种补充手段，与其他防御框架结合，提高模型在面对对抗性
	攻击时的鲁棒性。同时，这种方法在实际应用中具有简单易行和低计算成本的优点，可
	以方便地部署到各种应用场景。

	值得注意的是，本文提出的方法在高分辨率图像中表现出良好的检测性能，而不受攻击
	的先验知识或对抗性预算的影响。这意味着该方法具有一定的通用性，可以应对各种不
	同的对抗性攻击。此外，考虑到深度学习领域的快速发展，我们相信本研究为未来的研
	究提供了一个有益的方向，以进一步探讨对抗样本的本质和提高模型的鲁棒性。

	在现实世界的应用中，图像分类模型可能会面临各种来源的噪声和扰动，这些扰动可能
	会导致模型的性能下降。尤为重要的是，对抗性攻击可能会利用模型的薄弱环节，制造
	难以察觉的扰动来误导模型，从而对关键任务造成潜在的危害。因此，研究如何应对对
	抗性样本，提高模型的鲁棒性，已经成为了计算机视觉和深度学习领域的一项重要挑
	战。本研究的目标正是针对这一问题，提出了一种有效的对抗样本检测方法。这种方法
	可以帮助开发者和研究人员在实际应用中更好地了解和应对对抗性攻击，从而确保模型
	在面对这些攻击时仍能保持高水平的性能。

	在本研究中，我们还深入探讨了为什么现有的深度学习模型容易受到对抗性攻击的影
	响。通过对比不同的神经网络架构，我们发现网络的复杂性和容量可能与其对抗性攻击
	的易感性有关。此外，我们还分析了正则化方法，如权重衰减、Dropout 和 Batch
	Normalization 等，以了解它们在提高模型鲁棒性方面的作用。

	为了进一步证实本文提出的对抗样本检测方法的有效性，我们将其与其他已有的检测方
	法进行了比较。实验结果表明，本文的方法在准确率、召回率和 F1 分数等指标上均具
	有优越的性能。这些结果充分说明了本文方法在检测对抗样本方面的优势，并证实了其
	在实际应用中的可行性。

	以提高对抗样本生成和检测的能力；还有研究在训练过程中引入对抗性样本，从而使模
	型在学习期间就具备对抗性攻击的防御能力。此外，我们还关注到针对性能保护、隐私
	保护和模型泛化等方面的研究，这些研究成果可能对改进本文方法及提高模型鲁棒性具
	有指导意义。

	本研究的主要贡献在于提供了一种简单、高效且可扩展的对抗样本检测方法，以应对深
	度学习模型在实际应用中所面临的安全挑战。通过实验证明了该方法的有效性，并与现
	有方法进行了比较分析，为后续研究提供了宝贵的参考。

	在本研究中，我们成功地提出了一种简单、高效且可扩展的对抗样本检测方法。然而，
	我们认识到这项研究仍有一些局限性和未来的改进方向。

	\begin{itemize}
		\item 深入分析不同类型的对抗性攻击：虽然本研究涉及了一些常见的对抗性攻击
		      方法，但未来的研究可以考虑探索更多的攻击策略，以便更全面地评估所提
		      方法的鲁棒性。
		\item 扩展到其他任务和领域：本研究主要关注图像分类任务，但对抗性攻击同样
		      存在于其他计算机视觉任务，如目标检测、语义分割等。未来的研究可以将
		      所提方法应用于这些任务，以验证其在不同场景下的有效性。
		\item 研究更多的防御策略：除了本文提出的对抗样本检测方法外，还可以研究其
		      他防御策略，如对抗性训练、模型蒸馏和特征空间变换等。这将有助于构建
		      一个更强大、更全面的防御体系，进一步提高模型的鲁棒性。
		\item 模型可解释性与安全性：为了增强对抗性攻击的防御能力，可以研究提高模
		      型的可解释性，从而更好地理解其在面对对抗性样本时的行为。通过提高模
		      型的可解释性，我们可以揭示模型的薄弱环节，进而设计出更强大的防御策
		      略。
	\end{itemize}

	综上所述，本研究探讨了对抗样本的特性并提出了一种简单且有效的检测方法。通过实
	验验证了该方法的有效性，并为未来研究提供了有益的方向。我们相信，这项研究为解
	决计算机视觉和深度学习领域对抗性攻击问题迈出了重要一步，有望为实际应用中的模
	型提供更强大的安全保障。

	\keywords{深度学习；计算机视觉；对抗性例子}
\end{abstract}
\afterpage{\blankpage}

\begin{enabstract}
	\large
	Like other machine learning algorithms, neural networks have been vulnerable
	to adversarial examples, i.e., inputs containing specifically crafted
	perturbations whose only objective is to fool a network into
	misclassification. On the other hand, thanks to the continuous research and
	improvements made to neural network architectures, recent models are much
	more robust to random perturbations in the input. This robustness to random
	perturbations motivated the study of the effect of pixel-space perturbations
	on typical images (i.e., benign) and adversarial examples with varying
	intensities of noise and adversarial perturbation.

	These experiments, followed by the results described in this research, led
	me to propose a novel method to detect adversarial examples. The essence of
	the method is to deliberately introduce Gaussian noise of varying strengths
	on the input images to compute two metrics that evaluate the differences in
	predictions ante and post the added perturbations. Differences that I prove
	to be typically much higher with adversarial examples than with standard
	images using three popular benchmarks (Dogs vs. Cats, CIFAR-10, and
	ImageNet).

	The main advantages of my approach are its simplicity and low computational
	cost, making it easy to deploy and combine with other defense frameworks.
	Most importantly, the method shows high detection performance on various
	attacks on higher-resolution images regardless of any prior knowledge of the
	attack or its adversarial budget.
	\enkeywords{Deep Learning; Computer Vision; Adversarial Examples}
\end{enabstract}
\afterpage{\blankpage}
